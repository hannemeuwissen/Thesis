/**
 * @file poiss1d.c
 * @author Hanne Meuwissen (22307813)
 * @brief Code for assignment 1 for MPA55612 HPC Software II at Trinity College Dublin.
 * @version 0.1
 * @date 2023-03-12
 */
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <mpi.h>

/**
 * @brief Function that decomposes an array of doubles of size n across processors.
 * @param[in] n The length of the array.
 * @param[in] p The number of processors.
 * @param[in] myid The rank of the processor.
 * @param[in] s Pointer to store the address to the startpoint of the rank in the array.
 * @param[in] e Pointer to store the address to the endpoint of the rank in the array.
 * @return The function returns 0 on success, -1 otherwise.
 */
int decomp1d(int n, int p, int myid, int *s, int *e){
    if((n<1) || (p<1) || (p>n)){
        perror("Invalid input.");
        return(-1);
    }
    int n_elements = n/p;
    int remainder = n%p;
    *e = 0;
    for(int i=0;i<=myid;i++){
        *s = *e + 1;
        *e += ((i<remainder) ? (n_elements + 1) : (n_elements)); 
    }
    *s-=1;
    *e-=1;
    return 0;
}

/**
 * @brief Function that determines if the process is active in a certain step of the TSQR algorithm.
 * @param rank Rank of the calling process.
 * @param step Step in the TSQR algorithm.
 * @return int Returns 0 if the calling process is not active anymore, otherwise 1.
 */
int is_active(const int rank, const int step){
    return (((int) rank%pow(2.0, step)) == 0 ? 1 : 0);
}

/**
 * @brief Function that finds active process to send data to. 
 * @param rank Rank of calling process.
 * @param step Step in the TSQR algorithm.
 * @return int Returns the rank of the process it has to send its data to.
 */
int find_lower_active(const int rank, const int step){
    for(int i=rank-1;i>=0;i--){
        if(is_active(i, step)){
            return i;
        }
    }
    return 0;
}

/**
 * @brief Function that performs the communication avoiding TSQR using OpenMP.
 * @param[in] A The matrix that will hold the data. It will hold the final result for Q.
 * @param[in] M Number of rows of the matrix.
 * @param[in] N Number of columns of the matrix.
 * @param[in] R The matrix that will hold the result for R.
 * @param[in] rank The rank of the calling process.
 * @param[in] nprocs The number of processes.
 * @param[in] comm The MPI communicator
 */
void TSQR(double *A, const int M, const int N, double *R, const int rank, const int nrpocs, MPI_Comm comm){
    const int steps = log2(n);
    
    // NOW: all processes have local values of A, not entire matrix
    double * block, * tau, *tempA; // local variables!
    int start,end, inactive_proc_end;
    decomp1d(M, nprocs, rank, &start, &end); /* Partition M rows over processes */

    for(int step=0;step<=steps;step++){ /* Parallel TSQR loop */

        if(is_active(rank, step)){

            tau = malloc(N*sizeof(double)); /* Allocate space to hold tau's */
            tempA = malloc(N*N*sizeof(double)); /* Allocate space to temporarily hold local R's */

            lapack_int rows = ((!step)?end-start+1 : 2*N); 
            lapack_int cols = N;

            /* Calculate local Housholder QR in terms of tau and v's */
            LAPACKE_dgeqrf(CblasRowMajor, rows, cols, A, cols, tau); 
            
            /* Save R parts in temporary space for next iteration */
            for(int i=0;i<N;i++){ 
                for(int j=0;j<N;j++){
                    tempA[j + i*N] = ((i>j) ? 0 : A[j + i*N]);
                }
            }
            
            /* Get local Q */
            LAPACKE_dorgqr(CblasRowMajor, rows, cols, cols, A, cols, tau); 
            
            // IN STEP 0: PROCESS ACTIVE IN NEXT STEP MAKES NEXT BLOCK + receives R, end
            // IN STEP > 0: performs partial multiplication first + same things as in step 0
            if(step>0){ /* Perform part of multiplication for final Q */
                int qrows = inactive_proc_end - start + 1;
                double * res = malloc(qrows*N*sizeof(double));
                /* Perform local part of product */
                cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, qrows, N, 2*N, 1.0, block, 2*N, A, N, 0.0, res, N);
                if(step < steps){
                    if(is_active(rank, step + 1)){
                        // Make block that holds current res on top and will receive part from 1 other process
                        MPI_Recv(inactive_proc_end, 1, MPI_INT, MPI_ANY_SOURCE, 1, comm, MPI_STATUS_IGNORE);
                        int new_qrows = inactive_proc_end - start + 1;
                        block = malloc(new_qrows*2*N*sizeof(double));
                        memset(block, 0, new_qrows*2*N*sizeof(double));
                        for(int i=0;i<qrows;i++){
                            for(int j=0;j<N;j++){
                                block[j + i*2*N] = res[j + i*N];
                            }
                        }
                        // receive res from other process
                        realloc(res, new_qrows - qrows);
                        MPI_Recv(res, (new_qrows - qrows)*N, MPI_DOUBLE, MPI_ANY_SOURCE, 1, comm, MPI_STATUS_IGNORE);
                        for(int i=qrows;i<new_qrows;i++){
                            for(int j=0;j<N;j++){
                                block[j + N + i*2*N] = res[j + (i - qrows)*N];
                            }
                        }
                        // Receive R in A from other process
                        realloc(A, 2*N*N);
                        for(int i=0;i<N;i++){ 
                            for(int j=0;j<N;j++){
                                A[j + i*N] = ((i>j) ? 0 : tempA[j + i*N]);
                            }
                        }
                        MPI_Recv(tempA, N*N, MPI_DOUBLE, MPI_ANY_SOURCE, 1, comm, MPI_STATUS_IGNORE);
                        for(int i=0;i<N;i++){ 
                            for(int j=0;j<N;j++){
                                A[j + (i + N)*N] = ((i>j) ? 0 : tempA[j + i*N]);
                            }
                        }

                    }else{                      
                        int lower_active = find_lower_active(rank, step + 1);
                        // Send end to other process
                        MPI_Send(end, 1, MPI_INT, lower_active, 1, comm, MPI_STATUS_IGNORE);
                        // Send block part to active process 
                        MPI_Send(res, qrows*N, MPI_DOUBLE, lower_active, 1, comm, MPI_STATUS_IGNORE);
                        // Send R to A of other process
                        MPI_Send(tempA, N*N, MPI_DOUBLE, lower_active, 1, comm, MPI_STATUS_IGNORE);
                    }
                }else{ /* Replace A with last result */
                    set_equal(A, res, M, N);
                }
                free(res);
            }else if((step == 0) && (steps!=0)){ /* Place in block */
                // memset(block + start*2*N, 0, rows*2*N*sizeof(double));
                // for(int i=0;i<rows;i++){
                //     for(int j=0;j<N;j++){
                //         block[j + N*(rank%2) + (start + i)*2*N] = A[j + (start + i)*N];
                //     }
                // }
                if(is_active(rank, step + 1)){
                    // Make block that holds current res on top and will receive part from 1 other process
                    // Receive R in A from other process
                }else{
                    // Send block part to active process 
                    // Send end to other process
                    // Send R to A of other process
                }
            }
            // }
            free(tau);
            
            if(step == steps){ /* Get final R from temporary space */
                for(int i=0;i<N;i++){
                    for(int j=0;j<N;j++){
                        R[j + i*N] = ((i>j) ? 0 : tempA[j + i*N]);
                    }
                }
            }else{ /* Place R's in A for next local QR step */
                set_equal(A, tempA, N*active_procs, N);
                free(tempA);
            }
        }else{
            // send end, Q and R to active process with lower rank
            break;
        }

        // MPI processes should synchronize!
    }
    if(steps>0){
        free(block);
    }
}

int main(int argc, char **argv)
{  
    int myid, nprocs;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

    
    MPI_Finalize();
    return 0;
}